---
title: Segregate The Variability Climate Is Responsible For In Vegetation Loss Using
  Time Series Analsis.
author: 'Kalong Boniface And Fugah Seletey Mitchell '
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Loading Packages and Preparing The Datall0
library(tinytex)
```
### CHAPTER ONE

```{r, message=FALSE,results='hide',warning=FALSE}
# 
# #if(!require("pacman")){install.packages("pacman")}
# pacman::p_load(char = c('rgee','reticulate','raster','tidyverse',
#                 'dplyr','sf','mapview','mapeddit','caret','forcats','reticulate',
#                 'rgee', 'remotes', 'magrittr', 'tigris', 'tibble', 'stars', 'stars',
#                 'st', 'lubridate', 'imputeTS', 'leaflet', 'classInt', 'RColorBrewer',
#                 'ggplot2', 'googledrive', 'geojsonio', 'ggpubr','cartogram'), 
#                install = F, update = F, character.only = T)
```

```{r,results='hide'}
library(rgee)
library(sf)
library(reticulate)
#ee_install() 
ee_check()

ee_Initialize("kalong",drive = TRUE) # initialize GEE, 
#this will have you log in to Google Drive

```


```{r}
library('sf')
# Load shape file

#setwd("C:/Users/Guy/Documents/GitHub/Artisanal-Mining-In-Ghana-Galamsey/New Regions")
aoi <- read_sf('Ghana shp file/GHA/gadm41_GHA_1.shp')
aoi <- st_transform(aoi, st_crs(4326))
aoi.ee <- st_bbox(aoi) %>% 
st_as_sfc() %>% 
sf_as_ee() #Converts it to an Earth Engine Object


```


Now we will create a hexagonal grid over the study area
```{r}
library(tibble)
aoi.proj <- st_transform(aoi, st_crs(2392))
hex <- st_make_grid(x = aoi.proj, cellsize = 17280, square = FALSE) %>%
st_sf() %>%
rowid_to_column('hex_id')
hex <- hex[aoi.proj,]
plot(hex)
```

```{r}
terraclimate <- ee$ImageCollection("IDAHO_EPSCOR/MACAv2_METDATA") %>%
  ee$ImageCollection$filterDate("2000-01-01", "2022-01-01") %>%
  ee$ImageCollection$map(function(x) x$select("pr")) %>% # Select only precipitation bands
  ee$ImageCollection$toBands() %>% # from imagecollection to image
  ee$Image$rename(sprintf("PP_%02d",1:12)) # rename the bands of an image
```

```{r}
ee_nc_rain <- ee_extract(x = terraclimate, y = aoi.ee, sf = FALSE ,scale = 250)
head(ee_nc_rain)
```


```{r}
ee_nc_rain %>%
  pivot_longer(-NAME, names_to = "month", values_to = "pr") %>%
  mutate(month, month=gsub("PP_", "", month)) %>%
  ggplot(aes(x = month, y = pr, group = NAME, color = pr)) +
  geom_line(alpha = 0.4) +
  xlab("Month") +
  ylab("Precipitation (mm)") +
  theme_minimal()
```



Now we will use the grid created above to extract the mean EVI values within each cell for the years 2000-2020.


Which we are going to perform a time series analysis on the data within each grid cell. But first, we will work through the procedure one step at a time.
 
```{r,include=FALSE}
#Create an empty list, this will be used to house the time series projections for each cell.
evi.hw.lst <- list() 
#Create an empty list, this will be used to house the time series decomposition for each cell.
evi.dcmp.lst <- list() 
#This data frame will hold the trend data
evi.trend <- data.frame(hex_id = evi.df$hex_id, na.cnt = NA, NA_Values = NA, Trend = NA, P_value = NA, R_Squared = NA, Standard_Error = NA, Trend_Strength = NA, Seasonal_Strength = NA)
Dates <- data.frame(date = seq(as.Date('2001-01-01'), as.Date('2022-01-01'), "month"))

Dates$month <- month(Dates$date)
Dates$year <- year(Dates$date)
i <- 1
```

```{r}

#converting the data to a transposed data frame
tsv <- data.frame(evi = t(evi.df[i, 2:ncol(evi.df)])) 
colnames(tsv) <- c("evi")
#write.csv(tsv,"Data/tsv.csv")
head(tsv) #let's take a look
```
### CHAPTER THREE
### METHODOLOGY
        Data from a time series is a set of observations made in a particular order over a period of time. There is a chance for correlation between observations because time series data points are gathered at close intervals. To help machine learning classifiers work with time series data, we provide several new tools. We first contend that local features or patterns in time series can be found and combined to address challenges involving time-series categorization. Then, a method to discover patterns that are helpful for classification is suggested. We combine these patterns to create computable categorization rules. In order to mask low-quality pixels, we will first collect Sentinel 2 data from Google Earth Engine in order to choose NDVI and EVI values.
        
Instead of analyzing the imagery directly, we will summarize the mean NDVI and EVI values. This will shorten the analysis time while still providing an attractive and useful map. We will apply a smoothing strategy using an ARIMA function to fix the situation where some cells may not have NDVI and EVI for a particular month. Once NA values have been eliminated, the time series will be divided to eliminate seasonality before the normalized data is fitted using a linear model. We will go to classify our data on the map and map it after we have extracted the linear trend.
        
## Research Design
In this study, the submission used a quantitative approach. Instead of using subjective judgment, findings and conclusions heavily rely on the use of statistical methods and reliable time series models.
## Specification of the Model}
# Data Representation}
# The Analysis Of Variance (ANOVA) Method}
# The Empirical * Theory model}
# Assumptions }

```{r}
#We want to get an idea of the number of entries with no EVI value
na.cnt <- length(tsv[is.na(tsv)]) 
evi.trend$na.cnt[i] <- na.cnt
td <- tsv %>% 
  mutate(month = month(as.Date(rownames(tsv))), year = year(as.Date(rownames(tsv)))) %>% 
  group_by(year, month) %>%
  summarise(mean_evi = mean(evi, na.rm = T), .groups = "keep") %>%
  as.data.frame()
head(td)
```

That looks better! Unfortunately though, there are a number of dates which don't have any evi value at all, let's figure out which ones these are.

```{r, include=FALSE}

td$date <- as.Date(paste0(td$year, "-", td$month, "-01"))
dx <- Dates[!(Dates$date %in% td$date),]
```

```{r}
dx$mean_evi <- NA
tdx <- rbind(td, dx) %>% 
  arrange(date)
write.csv(tdx,"Data/tdx.csv")
tdx <- read.csv("Data/tdx.csv")
head(tdx)
```


```{r}
na.cnt <- length(tdx[is.na(tdx)])
# Convert data to time series.
tdx <- ts(data = tdx$mean_evi, start = c(2001, 1), end = c(2019, 11), frequency = 12) 
plot(tdx)
```
```{r}
library(imputeTS)
tdx <- if(na.cnt > 0){imputeTS::na_kalman(tdx, model = "auto.arima", smooth = T)} else {
    tdx
}
plot(tdx)
new_tdx <- write.csv(tdx,"Data/new_tdx.csv")
```
```{r}
tdx.dcp <- stl(tdx, s.window = 'periodic')
plot(tdx.dcp)
```
```{r,message=FALSE}
library(forecast)
Tt <- trendcycle(tdx.dcp)
St <- seasonal(tdx.dcp)
Rt <- remainder(tdx.dcp)
plot(Tt)
plot(St)
plot(Rt)
```
# Stationarity
When investigating a time series, one of the first things to check before building an ARIMA model is to check that the series is stationary. That is, it needs to be determined that the time series is constant in mean and variance are constant and not dependent on time.

Here, we will look at a couple methods for checking stationarity. If the time series is provided with seasonality, a trend, or a change point in the mean or variance, then the influences need to be removed or accounted for.
# Augmented Dickey–Fuller (ADF) t-statistic test for unit root
Another test we can conduct is the Augmented Dickey–Fuller (ADF) t-statistic test to find if the series has a unit root (a series with a trend line will have a unit root and result in a large p-value).
```{r,warning=FALSE}
library(tseries)
adf.test(Rt)
adf.test(Tt)
adf.test(tdx)
```
#Autocorrelation Function (ACF)
Identify if correlation at different time lags goes to 0
```{r}
plot.new()
frame()
# The Stationary Signal and ACF
plot(Rt,col= "red", main = "Stationary Signal")
acf(Rt, lag.max = length(Rt),
    xlab = "lag #", ylab = 'ACF', main = '')

#The Trend Signal anf ACF

plot(Tt,col= "red",main = "Trend Signal")
acf(Tt, lag.max = length(Tt),
    xlab = "lag#", ylab = "ACF", main = '')
```
It is noteworthy that the stationary signal (top left) generates few significant lags that are larger than the ACF's confidence interval (blue dotted line, bottom left). In contrast, practically all delays in the time series with a trend (top right) surpass the ACF's confidence range (bottom right). Qualitatively, we can observe and infer from the ACFs that the signal on the left is steady (due to the lags that die out) whereas the signal on the right is not (since later lags exceed the confidence interval).

```{r}
tdx.ns <- data.frame(time = c(1:length(tdx)), trend = tdx - tdx.dcp$time.series[,1])
summary <- summary(lm(formula = trend ~ time, data = tdx.ns))

```


        
```{r}
plot(tdx.ns)
abline(a = summary$coefficients[1,1], b = summary$coefficients[2,1], col = 'blue')
```

```{r}
# Count of na values to dataframe
# Calculating Trend and Seasonal Strength
evi.trend$NA_Values[i] <- na.cnt 
evi.trend$Trend[i] <- summary$coefficients[2,1]
evi.trend$Trend_Strength[i] <- round(max(0,1-(var(Rt)/var(Tt+Rt))),1)
evi.trend$Seasonal_Strength[i] <- round(max(0,1-(var(Rt)/var(St+Rt))),1)
evi.trend$P_value[i] <- summary$coefficients[2,4]
evi.trend$R_Squared[i] <- summary$r.squared
evi.trend$Standard_Error[i] <- summary$sigma
evi.trend[i,]
```

```{r}
plot(evi.hw <- forecast::hw(y = tdx, h = 12, damped = T))
```

```{r, eval=FALSE, include=FALSE}
if(readline(prompt = "Hit enter to proceed or type 'no' to download the data from G-Drive. ") == "no"){
googledrive::drive_download(file = googledrive::as_id("https://drive.google.com/drive/folders/1ZnCpYz38ezSU1XG7ixJ2sPg_DX7bO07J?usp=sharing"),overwrite = T)
evi.trend <- read.csv("evi_trend.csv")
} else {

for(i in 1:nrow(evi.df)){
tsv <- data.frame(evi = t(evi.df[i, 2:ncol(evi.df)]))
colnames(tsv) <- c("evi")
na.cnt <- length(tsv[is.na(tsv)])
evi.trend$na.cnt[i] <- na.cnt
if(na.cnt < 263){
td <- tsv %>%
  mutate(month = month(as.Date(rownames(tsv))), year = year(as.Date(rownames(tsv)))) %>%
  group_by(year, month) %>%
  summarise(mean_evi = mean(evi, na.rm = T), .groups = "keep") %>%
  as.data.frame()

td$date <- as.Date(paste0(td$year, "-", td$month, "-01"))
dx <- Dates[!(Dates$date %in% td$date),]

dx$mean_evi <- NA

tdx <- rbind(td, dx) %>%
  arrange(date)

na.cnt <- length(tdx[is.na(tdx)])
evi.trend$na.cnt.2[i] <- na.cnt
rm(td, dx)

tdx <- ts(data = tdx$mean_evi, start = c(2000, 1), end = c(2022, 1), frequency = 12)
tdx <- if(na.cnt > 0){imputeTS::na_kalman(tdx, model = "auto.arima", smooth = T)} else {
    tdx
}

 #This will save our decomposition plots
tdx.dcp <- stl(tdx, s.window = 'periodic')
evi.dcmp[[i]] <- tdx.dcp
png(filename = paste0(dir,"/decomp_plots/hw_", i,".png"), width = 1200, height = 650)
plot(tdx.dcp)

dev.off()
tdx.ns <- data.frame(time = c(1:length(tdx)), trend = tdx - tdx.dcp$time.series[,1])
Tt <- trendcycle(tdx.dcp)
St <- seasonal(tdx.dcp)
Rt <- remainder(tdx.dcp)

#tslm
trend.summ <- summary(lm(formula = trend ~ time, data = tdx.ns))
evi.trend$trend[i] <- trend.summ$coefficients[2,1]

#Trend Strength Calculation
evi.trend$trnd.strngth[i] <- round(max(0,1 - (var(Rt)/var(Tt + Rt))), 1)
evi.trend$seas.strngth[i] <- round(max(0,1 - (var(Rt)/var(St + Rt))), 1)

evi.trend$p.val[i] <- trend.summ$coefficients[2,4]
evi.trend$r2[i] <- trend.summ$r.squared
evi.trend$std.er[i] <- trend.summ$sigma
evi.hw <- forecast::hw(y = tdx, h = 12, damped = T)
evi.hw.lst[[i]] <- evi.hw

#This will save our projection plots
png(filename = paste0(dir,"/hw_plots/hw_", i,".png"), width = 1200, height = 650)
plot(evi.hw)
dev.off()
rm(evi.hw, trend.summ, tdx.ns, tdx.dcp, Tt, St, Rt, tdx, na.cnt)
gc()
} else {
  evi.ts[[i]] <- NA
    }
  }
}

head(evi.trend) #Let's take a peak
```
```


```{r}
#ggdensity(tdx, x = "tmie",y = "trend",fill = "#0073C2FF",color ="#0073C2FF",add = "mean",rug = TRUE)
```


### CHAPTER  FIVE
### CONCLUSIONS AND RECOMMENDATIONS
# Summary
            Broadly speaking, in this study we have presented a state-of-the-art of the following popular time series forecasting models with their salient features:
        
            -  The Box-Jenkins or ARIMA models for linear time series forecasting. 
            - Some non-linear stochastic models, such as NMA, ARCH. 
            - SVM based forecasting models; LS-SVM and DLS-SVM.
         
# Conclusions
It has been seen that, the proper selection of the model orders (in case of ARIMA), the number of input, hidden, output  and the constant hyper-parameters (in case of SVM) is extremely crucial for successful forecasting. We have discussed the two important functions. \textbf{AIC} and \textbf{BIC}, which are frequently used for \textbf{ARIMA} model selection. 
    
We have considered a few important performance measures for evaluating the accuracy of forecasting models. It has been understood that for obtaining a reasonable knowledge about the overall forecasting error, more than one measure should be used in practice. The last chapter contains the forecasting results of our experiments, performed on six real time series datasets. 
Our satisfactory understanding about the considered forecasting models and their successful implementation can be observed form the five performance measures and the forecast diagrams, we obtained for each of the six datasets. However in some cases, significant deviation can be seen among the original observations and our forecast values. In such cases, we can suggest that a suitable data preprocessing, other than those we have used in our work may improve the forecast performances. 
        #Recommendations}
        Time series forecasting is a fast growing area of research and as such provides many scope for future works. One of them is the Combining Approach, i.e. to combine a number of different and dissimilar methods to improve forecast accuracy. A lot of works have been done towards this direction and various combining methods have been proposed in literature [8, 14, 15, 16]. 
        Together with other analysis in time series forecasting, we have thought to find an efficient combining model, in future if possible. With the aim of further studies in time series modeling and forecasting


### References}

